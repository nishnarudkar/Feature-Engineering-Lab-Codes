{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMj/4RFaTgTRHwX/R/9jeuw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gysJrA0J3hRL","executionInfo":{"status":"ok","timestamp":1756707346527,"user_tz":-330,"elapsed":2200,"user":{"displayName":"Nishant Narudkar","userId":"01394224783039142192"}},"outputId":"8327ddac-7562-4e28-9662-a2e14cc5bdbb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized Documents:\n","Doc 1: ['the', 'cat', 'sat', 'on', 'the', 'mat']\n","Doc 2: ['the', 'dog', 'sat', 'on', 'the', 'log']\n","Doc 3: ['the', 'cat', 'chased', 'the', 'dog']\n","\n","Vocabulary:\n","['cat', 'chased', 'dog', 'log', 'mat', 'on', 'sat', 'the']\n","\n","Bag of Words Vectors (Manual):\n","Doc 1: [1, 0, 0, 0, 1, 1, 1, 2]\n","Doc 2: [0, 0, 1, 1, 0, 1, 1, 2]\n","Doc 3: [1, 1, 1, 0, 0, 0, 0, 2]\n","\n","Vocabulary (sklearn):\n","['cat' 'chased' 'dog' 'log' 'mat' 'on' 'sat' 'the']\n","\n","Bag of Words Vectors (sklearn):\n","[[1 0 0 0 1 1 1 2]\n"," [0 0 1 1 0 1 1 2]\n"," [1 1 1 0 0 0 0 2]]\n"]}],"source":["# Step 1: Sample unstructured text\n","docs = [\n","    \"The cat sat on the mat.\",\n","    \"The dog sat on the log.\",\n","    \"The cat chased the dog.\",\n","    \"The cat \"\n","]\n","\n","import string\n","# Manual Implementation\n","\n","# Step 2: Preprocess function\n","def preprocess(text):\n","    # Lowercase\n","    text = text.lower()\n","    # Remove punctuation\n","    text = text.translate(str.maketrans('', '', string.punctuation))\n","    # Tokenize\n","    tokens = text.split()\n","    return tokens\n","\n","# Apply preprocessing\n","tokenized_docs = [preprocess(doc) for doc in docs]\n","print(\"Tokenized Documents:\")\n","for i, doc in enumerate(tokenized_docs, 1):\n","    print(f\"Doc {i}: {doc}\")\n","\n","# Step 3: Build vocabulary (unique words)\n","all_tokens = [token for doc in tokenized_docs for token in doc]\n","vocabulary = sorted(set(all_tokens))\n","print(\"\\nVocabulary:\")\n","print(vocabulary)\n","\n","# Step 4: Create Bag of Words vectors\n","def vectorize(doc_tokens, vocabulary):\n","    return [doc_tokens.count(word) for word in vocabulary]\n","\n","vectors = [vectorize(doc, vocabulary) for doc in tokenized_docs]\n","\n","print(\"\\nBag of Words Vectors (Manual):\")\n","for i, vec in enumerate(vectors, 1):\n","    print(f\"Doc {i}: {vec}\")\n","\n","# Using scikit-learn CountVectorizer\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Step 5: Initialize vectorizer\n","vectorizer = CountVectorizer()\n","\n","# Step 6: Fit and transform the documents\n","X = vectorizer.fit_transform(docs)\n","\n","print(\"\\nVocabulary (sklearn):\")\n","print(vectorizer.get_feature_names_out())\n","\n","print(\"\\nBag of Words Vectors (sklearn):\")\n","print(X.toarray())\n"]}]}